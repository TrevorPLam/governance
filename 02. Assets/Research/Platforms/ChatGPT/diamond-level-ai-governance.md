Diamond-Level AI-Native Repository OS Extensions
System Overview
This Diamond-Level Repository OS extends an AI-first, solo-operator software factory with rigorous, automated governance. All code changes are AI-generated and funnel through mechanized controls instead of traditional human code review. One human remains “in the loop” only for risk-based exceptions. The goal is to replace manual code review with trust-by-verification – every change must carry proof of correctness and compliance.
Key Principles: Every repository contains self-descriptive manifests and policies that define what the AI can change, how changes are verified, and when human escalation is needed. Pull Requests (PRs) include a structured Change Contract documenting the intent, scope, risk, evidence, and rollback plan for the change. This contract serves as a “mechanized code review” that CI/CD pipelines validate against the actual diff. Mismatches (e.g. claiming a docs-only change while modifying code) cause an immediate failure. Automated quality gates – tests, static analysis, security scans, etc. – act as the guardians of merge. The system classifies changes by risk and type, using a Risk-Based Gate Matrix to decide if a PR can auto-merge or if human sign-off is required. Crucially, certain high-risk categories (security-sensitive changes, supply chain modifications) are never auto-merged and always trigger human review[1].
To handle necessary exceptions, a Waiver System is in place. When a check fails, the AI (or operator) can attach a waiver with a justification, severity, and expiry. Waivers are logged as explicit acceptances of risk, not silent bypasses. Policies define what can be waived (e.g. a test timing flake) and what cannot (e.g. a critical security vulnerability) – if a violation is non-waivable, the pipeline will fail outright. Over time, the system uses enforcement ratcheting: issues initially allowed with warnings or waivers will convert to errors as they recur, forcing permanent fixes. This prevents “forever waivers” and ensures that repeated problems trigger increasing enforcement (WARN → FAIL).
The repository itself is treated as an operating system with defined layers of control. Architectural boundaries (layering, module boundaries, domain rules) are enforced via static analysis and AI-assisted review against architecture manifests. The AI’s changes are continuously checked for entropy (e.g. introducing duplicate code or deviating from established patterns) and architectural drift. For example, if an AI attempt causes a presentation layer to call a database directly (bypassing a service layer), the policy-as-code checks will catch this boundary violation[2] and fail the build. Each repo contains required files like architecture manifests and decision records (ADRs), so the AI and the pipeline know the intended design and can catch deviations early.
Supply-chain integrity is built in at every stage. At PR time, dependency changes are detected and scanned; any new library with known critical bugs or licenses triggers a stop unless explicitly waived. At build/release time, an SBOM (Software Bill of Materials) is automatically generated to inventory components[3]. Release artifacts are digitally signed (e.g. using Sigstore cosign) and accompanied by provenance metadata. Deployment pipelines verify signatures and provenance before accepting artifacts – if an artifact isn’t from a trusted build or the SBOM shows unexpected components, the release is blocked. Minimal but strict rules ensure that solo developers get security benefits (like knowing “what’s inside” their software and that it’s unmodified[3]) without heavy overhead.
Finally, AI governance rules constrain the AI’s own behavior in code generation. The AI is allowed to write and refactor code, generate tests, and update documentation, but it must obey the repository policies (it sees the manifests and knows the do’s and don’ts). Forbidden actions include introducing secrets, duplicating large swaths of code, bypassing required reviews, or altering governance files/policies without approval. Every AI-authored change must come with evidence of correctness – e.g. new or updated tests or a recorded successful run[4] – and these evidentiary artifacts are attached in the Change Contract. All AI activity is auditable: the system logs the prompts, summarizations, and decisions involved in code changes, alongside the diffs and test results. This audit trail means any bug or drift introduced by AI can be traced back to the “conversation” or rule that led to it. In short, the human operator can always review why the AI did something and ensure there is no silent degradation of quality or deviation from project standards.
By combining these elements – change contracts, risk-based gates, waivers with history, a blueprint of manifests/policies, layered architecture enforcement, supply-chain security, and AI usage rules – the Diamond-Level Repository OS achieves a high-assurance, policy-driven development process. All recommendations here are concrete, machine-enforceable controls that make the repo self-governing. The human’s role shifts to defining policy and handling only the most critical exceptions, while trusting the automated system to enforce everything else. This specification now details each artifact and rule needed to implement this AI-native governance framework.
Artifact Definitions
Change Contract Template (Pull Request Contract)
Every PR must include a Change Contract that describes the change and provides the basis for automated trust. This contract can be a YAML or structured Markdown section in the PR description. Below is a template in YAML form, capturing the required fields:
change_contract:
  intent: "Short summary of what the change does and why."
  scope: "Scope of changes (modules, files, or features affected)."
  risk_classification: "Low/Medium/High – based on impact (e.g. High if security or critical functionality)."
  required_evidence:
    - "Tests: e.g. Added unit test verifying X scenario passes."
    - "Evidence: e.g. Screenshot of feature working or log output of successful run."
  rollback_strategy: "Plan to rollback or mitigate if this change fails (e.g. revert PR, feature flag toggle)."
Usage: The AI (or solo developer) must fill this out before merging. It is effectively a self-review checklist. Key elements include the intent/why of the change, what parts of the system it touches (scope), and a risk classification that maps to governance rules (see Gate Matrix below). The required evidence list forces the author to prove the change works – e.g. reference to new tests, test results, or manual verification. A rollback strategy is included so even an AI knows how to undo or mitigate the change if it causes problems in production.
This PR Contract formalism is derived from emerging best practices for AI-assisted development. For example, teams using AI require PR descriptions to state intent, show proof (tests/logs), and note risk level[5]. By mandating “proof over promises”[4], the contract ensures no code is merged without demonstration of correctness. It replaces the human code reviewer’s question, “Does this actually work and what should I look at?” with a machine-checkable form.
CI Validation: The CI pipeline validates the Change Contract against the actual PR diff: - Intent/Scope vs Diff: The declared scope (e.g. “only updates docs” or “changes module X”) is checked against the files changed. If files outside the described scope are modified, or the intent doesn’t match the diff, the pipeline fails the PR (mismatch between description and reality). For example, claiming a “refactor with no behavior change” but editing a security configuration would fail. - Risk Classification vs Content: If the risk classification is understated (e.g. marked “Low” but the diff includes changes to authentication logic), a policy rule flags it. Certain directories or file patterns can automatically bump risk (e.g. anything in security/ folder must be at least High risk). A misclassified PR fails or at least warns for human escalation. - Required Evidence: The pipeline checks that each evidence item is provided. For instance, if the contract says “Added unit test,” it will verify that test files were indeed modified/added in the diff or that test results in CI include that test’s output. If evidence is missing or the new tests do not actually pass, the PR is blocked. No PR is allowed to merge without new tests or a demo proving the change works[4]. - Rollback Strategy: The contract must have a non-empty rollback plan. While this is text and cannot be auto-verified for correctness, an empty or placeholder rollback field fails the contract check. Optionally, patterns like “revert” or “feature flag” can be encouraged. This ensures the author has considered recovery; if the field is blank, CI rejects the PR as incomplete.
The Change Contract, once validated, becomes an auditable artifact. It can be stored in the repo (as part of merge commit message or a YAML attached to the PR) so that any future maintainer or AI agent can see why a change was made and what assurances accompanied it. This contributes to knowledge sharing and serves as a “spec for the commit” akin to documentation of intention.
Risk-Based Gate Matrix
Not all changes are equal. The Gate Matrix defines which checks are required for each change type and whether a PR can be auto-merged by the AI or must wait for human approval. The matrix ties together change type, CI checks, auto-merge eligibility, and mandatory human-in-the-loop (HITL) conditions. Every PR’s risk_classification and scope (from the change contract) inform its type.
Change Type	Required CI Checks	Auto-Merge?	Mandatory Human if…
Documentation	Spellcheck, linting, site build check	Yes, if all checks pass	Human only if touches user guides for legal/compliance text
App Logic (Code)	Unit/Integration tests, static analysis (lint), performance test (if relevant)	Conditional – auto-merge if tests + lint green, risk=Low/Med	If risk=High (critical module) or test coverage drops, require human review
Infrastructure/IaC	IaC lint/validate (e.g. Terraform plan), config syntax check, relevant unit tests	Conditional – auto-merge if non-prod changes and all checks green	Changes to production infra or CI pipeline require human sign-off
Security-Sensitive	Full test suite, SAST security scan, dependency audit	Never Auto-Merge	Always – any auth, encryption, secrets, or vulns involved triggers human review[1]

Supply Chain	Dependency diff analysis, license compliance check, SBOM generation preview	No (for new dependencies or build system changes)	Always for new third-party additions or build scripts (ensure review for trust)
Configuration	Config validation, all regression tests on affected features	Yes, if tests pass and risk Low (e.g. toggling feature flag)	Human if changing critical settings (e.g. security config, off by default to on)
Explanation: This table encodes the decision logic for merging: - Documentation changes are low-risk and can auto-merge once a simple check (like spellchecker or docs site build) passes. The only time a doc change might need human eyes is if it’s a sensitive policy document or legal text (in solo context, likely not applicable). Otherwise, docs don’t require the human gate. - Application Logic (feature code, bug fixes) runs through the full battery of tests and linters. If everything passes and the change isn’t deemed “High” risk, the AI can merge it automatically. However, if the change is in a critical area (e.g. core payment processing code) or if it causes a notable drop in test coverage or new warning, it is marked High risk and must wait for human approval. Essentially, normal code changes can be autonomous when quality signals are all good, but anything fishy or critical bumps it to manual review. - Infrastructure or IaC (infrastructure as code, deployment scripts) changes are treated carefully. Non-production infra (like updating dev environment configs) can auto-merge with validation checks. But anything affecting production, CI/CD pipeline, or permissions is flagged for human review. This prevents the AI from inadvertently altering deployment processes or access controls without oversight. - Security-Sensitive changes are never auto-merged by policy. If a PR touches authentication flows, secret keys, encryption, or fixes a known vulnerability, it must be reviewed by the human operator and likely subjected to extra scrutiny (threat modeling, pen-test tools)[1]. The matrix explicitly marks these as “Always HITL.” Even if tests pass, the risk is too high to trust AI-alone due to the high rate of AI introducing subtle security flaws[6]. The pipeline will block merging until a human signs off after doing a deep review and perhaps additional scanning. - Supply Chain changes involve adding or updating dependencies, or modifying build tooling. These are high impact on trust. If a new library is introduced, the system should not auto-merge it until a human verifies the library’s credibility (or at least the AI runs extensive checks). The required checks include diffing the dependency list, running a vulnerability scan (SCA) on new components, and checking licenses. For instance, if the AI adds an NPM package, the CI could run npm audit or consult a vulnerability database. Auto-merge is disabled for such PRs by default; the human needs to approve that the new dependency is acceptable or the build script change is safe. This guards against malicious packages being slipped in (a real threat in solo projects[7][8]). - Configuration changes (like toggling feature flags, changing environment variables, updating non-code config files) are usually low risk if they pass all tests – the system can auto-merge a config default tweak that doesn’t break any test. However, if the config relates to something critical (enabling an experimental feature globally, or adjusting memory limits), it might warrant manual review. The policy can say, for example: if a config file named security.yml is touched, treat it as Security-Sensitive (never auto). If it’s a feature flag, allow auto if tests show it’s okay.
The “Auto-Merge Eligibility” column defines the conditions under which the PR can be merged by the automation. The “Mandatory HITL” column clarifies the triggers that override auto-merge and force a human in the loop. These rules are implemented in CI as conditional checks (and can be encoded in branch protection or a merge bot like Mergify with conditions).
For example, an automated rule might be: IF change_type == Security OR risk_classification == High THEN label “manual-review” and disable auto-merge. Another rule: IF all required checks pass AND change_type allows auto AND risk_classification == Low/Med THEN auto-merge the PR. This matrix provides a clear “never auto-merge” list – any PR falling in those categories will not merge without human approval, period.
Waiver Schema and Control Plane
The Waiver System provides a structured way to override certain failures while maintaining accountability. A waiver is an artifact that grants a temporary exception to a policy or failing check. We extend the existing waiver mechanism into a first-class control plane – meaning waivers are tracked, audited, and themselves subject to rules.
Waiver Schema: Each waiver is defined with the following key fields (as a YAML record or a database entry):
•	ID/Control: Identifier of the rule or check being waived. (E.g. a test case name, a static analysis rule ID, or a policy name that is being exempted.)
•	Severity: The severity level of the violation or risk being waived. Common levels might be Critical, High, Medium, Low. This indicates how serious the issue is (for auditing purposes). Example: waiving a Low-severity code style lint vs waiving a High-severity security scan failure.
•	Justification: A mandatory free-text explanation of why the waiver is needed. This should detail why it’s acceptable to override the check and what the plan is (if any) to resolve it. (E.g. “Test is flaky in CI, waiving until we fix the flakiness” or “False positive security scan for known safe code”).
•	Expiry: An expiration timestamp or duration for the waiver. After this time, the waiver lapses and the check will be enforced again. If no expiry is set, it might default to a short period or require explicit renewal. (Permanent waivers are discouraged; if absolutely needed, they must be clearly marked.) For example, Chef InSpec waivers use start/end dates and treat no end-date as a permanent waiver[9].
Optionally, additional fields can be included: - Waiver Level: Whether this is an automatic (pre-approved low-impact) waiver vs. manual (human-approved). In a solo context, the human approval is implicit, but this can mark if the system applied a default waiver. - Issue Link: Link to a tracking issue or ADR if the waiver relates to a known problem or decision. - Renewals: A count of how many times this waiver has been extended or renewed (helps in ratcheting logic).
A sample Waiver entry in YAML might look like:
waivers:
  - id: "lint.rule.MAX_LINE_LENGTH"
    severity: Low
    justification: "Legacy code has lines >120 chars; waiving until auto-format is applied."
    expires: "2026-03-01T00:00:00Z"
    renewed: 0
All waivers are stored (e.g., in a waivers.yaml in the repo or a database) and fed into the CI pipeline so that when a check fails, the pipeline can see if a valid waiver exists for it. If so, the pipeline logs it and skips failing that check (treating it as warn).
Waiver Rules: The control plane enforces several policies around waivers:
•	What Can vs Cannot Be Waived: Certain classes of checks are non-waivable. For example, any hard security prohibition (like secret detection) or a failing high-severity security test cannot be waived; the build must stop. Similarly, a compilation error or failing unit test for core logic might be non-waivable because proceeding could break the build. Waivable items are generally those that do not pose an immediate critical risk or have a justified reason. For instance, style guide violations, known minor bugs, or non-critical test failures can be waived with justification. The system might maintain an allow-list of waivable rule IDs. If someone attempts to waive a disallowed item, the waiver will be rejected by CI.
•	Approval & Scope: In this solo scenario, the single human approves waivers, but we still enforce scope. A waiver should be as narrow as possible – e.g., waiving a specific test on a specific OS, rather than “ignore all tests.” The waiver must indicate the scope (which project or component it applies to, analogous to how Sonatype allows waivers at various scopes[10]). Broad waivers (org-wide or entire policy) are strongly discouraged or disabled.
•	Expiry & Renewal Limits: Every waiver should have a timer. Default policy might set maximum 14 days for High risk, 30 days for Medium, etc., to force revisiting the issue. Renewals (extending a waiver) are limited – e.g., a given waiver can only be renewed once or twice. This prevents the AI/human from continuously punting on an issue. If a waiver expires and the issue is still present, the next build will fail (or at least escalate to required human attention), forcing resolution or an explicit new waiver with justification. The system logs each renewal and can refuse renewal if it exceeds limits (e.g. “policy X cannot be waived more than 2 consecutive releases without an audit”).
•	Escalation on Repeated Waivers: The platform tracks waiver history. If the same violation is waived repeatedly (across multiple PRs or releases), it triggers an escalation. For example, if a test “LoginTest” was waived in the last 3 releases, the system could flag this for human review or even auto-create a high-priority task to fix the underlying issue. Repeated waivers indicate either technical debt or negligence, so the OS might enforce that after 3 waivers, the fourth occurrence fails the build outright until fixed. This history-based tightening ensures issues don’t linger indefinitely under a waiver blanket.
•	Ratchet Logic (WARN → FAIL): New policies or rules can start in “monitor” (warn) mode and ratchet up to “enforce” (fail) mode over time. For example, when introducing a new code complexity limit, the first month the pipeline might only warn if complexity is too high (and auto-create a waiver internally). After a grace period, the waiver expires and the same condition now fails the build. The OS can automate this by timestamp – e.g., rules have an “effective date” after which they switch from warn to fail. Similarly, any individual waiver ratchets: the first time an issue appears, it might warn and auto-waive, the second time require explicit waiver, the third time fail (no waiver allowed). This implements the governance ratchet: tolerance for issues decreases over time to drive continuous improvement. All ratcheting events are logged as part of waiver history.
Overall, the waiver system provides flexibility for the solo dev to keep moving fast (not every minor issue blocks merges) but with controlled, temporary exceptions. By automating waiver tracking, the system maintains an evidence trail of accepted risks and ensures they are revisited. Waivers are not hidden or ad-hoc; they are first-class artifacts subject to policy (just like code). This means the repository contains a clear record of any known deviations from standards, why they were allowed, and when they must be resolved – effectively turning “a blind eye” into a managed process.
Repository OS Blueprint (Manifests, Policies, Profiles)
Think of each repository as not just code, but a governed system with mandatory structure and configuration – a “Repo Operating System.” This blueprint outlines the files and configurations every AI-managed repo must include:
•	Manifest Files: These are machine-readable specifications that declare the intended state and rules of the repo. For example:
•	An Architecture Manifest (e.g. architecture.json or layers.yml) defining the allowed dependency directions between modules (which layer can import which). This might enumerate modules or directories and list which ones they may depend on. The AI and static checks consult this to prevent layer violations.
•	A Policy Manifest (e.g. policy.yml) summarizing key repo policies – e.g. coding standards, maximum complexity thresholds, test coverage minimum, etc. Some policies might be embedded in tool configs (like linters), but this file provides a high-level collection of governance switches (like “enforce_no_secrets: true”).
•	A Verification Profile (e.g. verification.yml) which lists what verification steps apply for different risk levels or components. For instance, it might specify that for a High-risk change, in addition to unit tests, a penetration test or fuzz test must run. It could map risk classifications to additional pipelines or test suites. The manifest-driven execution ensures that depending on context, the correct checks run (if a new microservice is added, the manifest would include its verification needs).
These manifests allow declarative governance. The AI can read them to understand constraints, and CI pipelines can load them to know what to enforce. For example, if architecture.yml says UI -> API -> Service -> Database layering, an automated architecture linter can parse that and catch any code that breaks it (like UI calling Database directly, flagged as violation[11]).
•	Templates: The repository contains templates that scaffold consistent processes. Key templates include:
•	Pull Request Template – essentially the Change Contract described above, likely in .github/PULL_REQUEST_TEMPLATE.md, so every PR description auto-includes the required sections for intent, scope, etc.
•	Issue Template – if using issue tracking, to ensure any bug/feature has necessary info (could tie into AI’s context when generating changes).
•	ADR Template – a template for Architecture Decision Records, if the project uses them, ensuring each decision doc has rationale, date, and status.
•	Coding Templates – optional snippets or file templates the AI can use for consistency (e.g. new module skeletons that include standard logging, etc.), though this is more about development speed than governance.
•	Policies as Code: In addition to manifest files, there are configuration files for enforcement tools:
•	Linters (e.g. .eslintrc, .flake8, etc.) to enforce style and simple rules.
•	Static analysis configs (e.g. CodeQL config, or custom rulesets).
•	Test coverage thresholds (some projects have a config file for minimum coverage %).
•	Dependency allowlist/denylist (e.g. a file listing approved open-source licenses or known-bad dependencies).
•	AI assistant rules (if supported, e.g. a file that lists banned suggestions or preferred patterns for the AI, akin to the .cursorrules mentioned in some contexts[12]).
These files live in the repo so that policy is version-controlled alongside code, following a CodeOps philosophy where governance rules are code[13]. For example, an architecture/policies/ directory might contain microservices.yaml with rules for service communication, which an AI agent uses to evaluate PRs[14].
•	Enforcement Layers: The blueprint defines that enforcement happens at multiple layers:
•	Pre-Commit/Pre-push Hooks (Local or AI agent): Optional, but if present, these run basic linters or tests before code even leaves the dev environment. The AI coding agent can self-check its output here (e.g. auto-run unit tests it generated).
•	CI Pipeline (PR checks): This is the main enforcement layer – running all required tests, checks from manifests, validating the change contract, etc. It uses the manifests and policies above to decide what to run. For instance, the CI references the risk from the PR contract and loads the verification.yml to run appropriate test suites (fast tests for low risk, full battery for high risk).
•	CD/Release Gates: Enforcement continues at release time – e.g. verifying SBOM, running integration or smoke tests in a staging environment, and ensuring signatures/provenance. The blueprint might require a deploy.json manifest that outlines promotion criteria (e.g. “must pass load test in staging”, “no Critical vulns open”).
•	Runtime/Monitoring Feedback: (Beyond the scope of code repo, but for completeness, one could include that production monitoring feeds back issues, which might mark something as high risk next time.)
•	Required Files in Every Repo: Here is a list of essential files every governed repo should contain, with their purpose:
•	README.md – Overview of the project (for human context; ensures the AI and others understand the project intent and usage).
•	SPEC.md – Product/Feature specifications. A document (often drafted by the AI and approved by human) describing what the software should do. This guides the AI’s development and acts as a reference to prevent feature creep. (Addy Osmani notes starting projects with a spec that the AI drafts and the human approves[15]).
•	CHANGE_CONTRACT.md (or PR Template) – Pull Request Change Contract template. As described, ensures every PR has the structured contract. If not in a standalone file, then at least in .github/PULL_REQUEST_TEMPLATE.md.
•	architecture.yml / ADR/ directory – Architecture description. architecture.yml could list subsystems and allowed dependencies (for automated checks). Additionally, an /adr/ folder or a DECISIONS.md file logs Architecture Decision Records – each major design decision the human makes (or reviews). This is crucial to provide context to the AI to avoid pattern drift (the AI needs to know which patterns are deprecated or preferred[16]). For example, DECISIONS.md might say “We use library X for auth instead of Y as of 2024; do not reintroduce Y.” The pipeline or AI can parse this to forbid suggestions that go against those decisions.
•	.github/workflows/ – CI/CD pipeline definitions. These YAML files define the automated checks (tests, builds, etc.) to be run on PR and on release. They effectively codify the gate matrix and verification profiles. Every repo should have a CI workflow that enforces tests and the policies described.
•	policy.yml / .repo-policy/ – Repository governance settings. Could be a single YAML or a folder containing various policy-as-code scripts (like OPA/Rego policies, or custom scripts). For instance, a no-secrets.rego policy could assert that no file matches a secret regex (with a hard fail).
•	waivers.yaml – Waiver log. A file (or directory of files) that lists active waivers (as per schema above). This ensures waivers are tracked in version control or at least easily accessible. In some implementations, each waiver might be a small YAML in a .waivers/ folder, one per issue, which is easier to manage than one big file.
•	DEPENDENCIES.md / SBOM.json – Dependency and supply chain info. The repo should either maintain a manual list of core dependencies and their update policy (for awareness), or simply rely on generating an SBOM at build. While an SBOM is generated during CI, having an SBOM artifact attached to each release (or commit) is recommended. Some projects commit an sbom.spdx.json for transparency, but at minimum it’s a build artifact. The key is that the blueprint mandates an SBOM be produced for each release[17] (and optionally signed).
•	TESTS/ – Test suite. Not a single file, but the presence of a test directory with adequate tests is a requirement for any repo. The OS might enforce that any new code has corresponding tests (possibly using a policy that checks for code-to-test ratio or modified files have modified tests).
•	SECURITY.md – Security policy. (Mainly if the project is open source – describes how to report vulns.) For our context, not crucial for AI, but could include known security requirements that the AI must adhere to (e.g. “All secrets must come from env, never hardcoded” – which can be enforced).
•	CODEOWNERS – In multi-person teams, this would route PRs to owners. In solo context, this might not be needed (the one operator is the owner of all). But could still be used to make the human required reviewer for certain paths (simulate CODEOWNERS enforcement for critical files – effectively the human will always be requested on security area changes, aligning with our matrix).
•	.git/hooks/ (or lint-staged config) – Local hooks configurations. Possibly included to encourage local checks for those running a local dev setup with the AI.
All these files and templates work in concert: the manifest and policy files tell the AI and CI what the rules of the game are; the templates ensure consistent documentation of changes; the ADRs and spec give context to avoid missteps; the pipeline files enforce execution of tests and checks; and the presence of waivers.yaml and SBOM signals that compliance and security are first-class concerns.
By blueprinting the repo structure, a new AI-generated project starts with a governance scaffolding out-of-the-box. The AI can be instructed to always include and update these files. For example, when adding a new module, it should also update architecture.yml to include it, and perhaps add an ADR if it’s a major decision. The repository is thus self-descriptive: both humans and AI agents can quickly determine the rules (by reading the manifests) and the state of compliance (by reading waiver logs, test results, etc.).
Control Rules (Automated Decision Logic)
This section defines explicit IF/THEN rules that the Repo OS enforces. These rules are encoded in automation (CI scripts, bots, or policy-as-code) to make merge decisions, apply waivers, or block certain actions. They implement the policies behind the artifacts above.
Change Contract vs Diff Validation Rules
•	If a PR is opened without a Change Contract (missing required sections or file), then the CI fails the PR checklist (the PR cannot be merged until the contract is provided).
•	If the intent or scope stated in the Change Contract does not match the files changed in the diff, then fail the build. <small>(Example: Contract says “update README only”, but code files are modified – CI will flag scope mismatch.)</small>
•	If risk_classification is set to Low but the changed files include any path tagged as high-risk (e.g. /auth/ directory or critical components), then override classification to High and require the high-risk process (CI can add a label or environment variable triggering stricter gates).
•	If required evidence is declared in the contract but corresponding proof is missing (no test file changes, no attached screenshot, or tests did not actually pass in CI), then block merge. Conversely, if evidence items are all present and successful, then mark the contract check as passed.
•	If the rollback_strategy field is empty or a placeholder (e.g. “N/A”), then fail the contract check – every change must have a rollback noted.
•	If any structural violation is found in the contract format (missing fields, unknown risk level, etc.), then fail CI and prompt the AI to fix the contract format.
Merge Gate Rules (Auto-Merge Logic)
•	If all required status checks for a PR pass and the change type is eligible for auto-merge per the Gate Matrix (see above), then the PR is automatically merged by the bot. (E.g., if a docs-only change has ✔️ on spellcheck and link check, the bot merges it.)
•	If the PR’s change type or risk level falls under “never auto-merge” (Security-sensitive, new dependency, etc.), then any attempt to auto-merge is prevented (the bot will not merge, and may apply a “manual review required” label). Instead, then require explicit human approval on that PR.
•	If a change is categorized as High risk, then require an approved review from the human (even if tests pass) and do not auto-merge; CI can enforce this with branch protection rules (e.g. a GitHub branch rule requiring “Owner approval” label for high-risk PRs).
•	If critical checks are failing (e.g. tests) and no waiver exists (see waiver rules), then obviously do not merge – the PR remains unmergeable.
•	If a PR is approved by the human before all checks are green (i.e., human override), then still block merge unless a waiver is filed for the failing checks. (This ensures that even the human can’t accidentally merge red builds without formally acknowledging via a waiver.)
•	If a change only modifies files deemed trivial (config values, docs, comments) and passes a minimal test set, then allow auto-merge bypassing full review – but tag the commit as such for audit. (This rule helps speed up non-critical tweaks.)
•	If multiple PRs are queued for auto-merge, then the system merges them one at a time, re-running quick tests between merges to catch any integration issues (to avoid compounding errors).
•	If the target branch is protected by sequential gates (e.g. require staging tests after merge), then merging is only final after those pass (and if they fail, the change is rolled back automatically via the rollback plan).
Waiver Rules (Exception Handling Logic)
•	If a pipeline check fails and a corresponding active waiver (ID matches and not expired) is found, then do not fail the build for that check – log a warning that “Check X failed but waived until Y date.” The overall CI can still pass in this case.
•	If a check fails and no valid waiver exists for it, then fail the pipeline as normal (the issue must be addressed or explicitly waived to proceed).
•	If a waiver is provided for a disallowed category (non-waivable rule), then the waiver is rejected by the system (CI fails with message “Cannot waive rule X – waiver policy prohibits it”). E.g., if someone tries to waive a “secret scan failure,” CI will refuse.
•	If a waiver’s severity is Critical (meaning we’re accepting a high-criticality risk), then always require human approval of that waiver. (In practice, the solo dev would have to sign off their own waiver via a special step or an “acknowledge” command.) For lower severities, auto-approval can be allowed if within policy.
•	If a waiver has expired (current date > expiry), then treat it as non-existent – the next time that issue occurs, it will be a failure unless a new waiver is issued. Additionally, CI can notify that “Waiver for X expired; issue must be resolved or waiver renewed.”
•	If a waiver is about to expire (e.g. will expire in the next week) and the underlying issue is still present in code, then surface a warning to the human to either fix the issue or renew the waiver. This can be done via an issue or notification.
•	If the same violation is waived more than N times in a row (e.g. N=2 or 3), then escalate: for example, after 3 waivers, the 4th occurrence fails the build outright despite any further waiver, effectively forcing a fix. (The threshold N and behavior can be configured, but the principle is to ratchet down tolerance.)
•	If an incoming PR introduces a new policy violation that is not yet in the codebase, and if that policy is in a “grace period” (warn-only mode), then auto-generate a one-time waiver with short expiry (or mark as warning) and post a notice (so that next time it will fail). Essentially: new rule violations are temporarily waived but recorded, to give time to adapt.
•	If a waiver is created, then log it in the waivers.yaml (or dashboard) with timestamp and who/what created it (AI or human). All waivers must be traceable.
•	If an expired waiver is found still in the log, then optionally remove or archive it for clarity, and possibly open a tracking issue to ensure the underlying risk was dealt with.
•	If any attempt is made to bypass a check without a waiver (for instance, force merging with admin rights), then treat it as a policy violation event – log it and require post-factum justification. (In solo context this is just a reminder; in teams it’d trigger audit.)
These waiver rules ensure that exceptions are always explicit and bounded. The automation enforces that enforcement can only be relaxed in controlled ways – either via a logged waiver or not at all.
Architectural Boundary Rules
•	If the code changes introduce a dependency that violates the declared architecture layering (as per architecture.yml or ADRs), then the static analysis (or AI check) flags it. For example, if a UI layer class imports a database repository class directly (bypassing the service layer), then fail the “architecture check” step of CI[2].
•	If any unauthorized dependency is added – e.g. using a library not on the approved list or calling an internal API that should be abstracted – then block the PR until this is addressed or explicitly waived[2]. (An “unauthorized dependency” could mean the AI added a third-party utility that’s not been vetted, or a microservice calls another directly when it should use an event bus, etc.)
•	If duplicate code or logic is detected (e.g. the AI writes a function that already exists elsewhere), then flag it and fail the duplication check. Duplicate detection can be done via a similarity scan or even AI-based detection of semantic similarity. For instance, two functions with 90% identical code would trigger this. The rule might be “No new code with >80% similarity to existing code is allowed without justification.” If triggered, the AI should refactor to reuse the existing code instead[18].
•	If code complexity metrics exceed the predefined budget, then fail the quality gate. For example, if the manifest says “Function complexity must not exceed 10 cyclomatic complexity or 50 lines,” and the AI produces a 100-line function with CC=20, the CI will flag it. The AI must then split or simplify the logic. Complexity budgets can also apply cumulatively: if a PR increases the overall code complexity score beyond a threshold, then require refactoring.
•	If the change crosses a domain boundary that is allowed only via an approved protocol, then enforce that protocol. For example, if microservice A should not call microservice B’s database, but the PR adds such a call, fail it – the rule is “services communicate via API only.” Only if an approved exception exists (like an ADR allowing this specific case) can it proceed. In that case, the PR must reference the ADR or exception token.
•	If an architectural rule must be broken for a valid reason (a “boundary-break” scenario), then require a specific protocol:
•	The AI/human must create an ADR or record explaining the deviation.
•	A temporary waiver is issued for this architecture violation, possibly tied to a future refactoring plan.
•	The exception is logged (so it’s not silent). E.g., if a quick fix requires a layer violation, then commit message must include “ARCH_EXCEPTION: [ADR-0012]” and CI will allow it only if that reference exists.
•	If the codebase shows signals of architectural drift (lots of small violations or erosion of boundaries over time), then surface a report to the human. (This might not fail a single build but could be a periodic check.) For instance, if number of dependency cycles or layer violations is increasing, then alert that architecture is decaying.
•	If an AI agent is reviewing architecture (optional advanced step), then it should parse current diagrams/ADR vs code. If it finds inconsistencies (like a data flow in code not documented in architecture), then it posts a warning or opens an issue to update docs or reconsider design[19].
•	If certain patterns are banned by prior decisions (e.g. “Do not use Object.assign” per team preference or “avoid XYZ pattern” per ADR), then static analysis or AI lint checks the diff for those patterns. If found, then fail and instruct to use the sanctioned approach[12].
•	If a file or module is designated as frozen or highly sensitive (e.g. core cryptography module), then any change to it triggers a special review (like require human approval always, similar to security rules).
•	If the project has a complexity budget for certain layers (e.g. UI layer code < X lines, or no more than N React components), then any PR exceeding that budget fails until the author reduces complexity or gets an approval to extend the budget (which might be an ADR decision).
•	If two modules start to duplicate each other’s functionality (AI created a similar feature in two places), then flag it in code review – might not auto-fail, but warn that consolidation is needed.
These rules mechanically enforce architecture consistency. They rely on either static code analysis tools configured with the architecture manifest (such as ArchUnit for Java, or ESLint rules for module boundaries in TypeScript, etc.), or AI-based policy agents scanning diffs for high-level issues (like an LLM noticing a violation in coupling). The “sanctioned boundary-break protocol” ensures that if we break the rules, it’s done consciously with documentation and (ideally) a plan to fix it later, rather than accidentally.
Supply Chain Rules
•	If a PR adds or updates a dependency (detected via changes in package.json, requirements.txt, go.mod, etc.), then trigger a supply chain check:
•	The system generates or updates the SBOM and diffs it to identify new components[17].
•	It runs a vulnerability scan (SCA) on the new/updated components.
•	It checks license compliance (if a new library has a disallowed license, fail).
•	If the vulnerability scan finds any critical or high severity issues in the new dependency (or an updated version), then fail the build (or at least mark as high risk needing human sign-off). The only way to proceed is to either choose a different version/dependency or provide a waiver with justification (e.g. “No alternative for this functionality, accepting risk temporarily” – but that would likely be High severity so strong justification needed).
•	If a new dependency’s metadata (author, source) is suspicious (e.g. package with no stars, or known typo-squat name), then warn or fail pending investigation. The AI should prefer well-known libraries, but this rule is a backstop against it grabbing a potentially malicious one by name confusion[20].
•	If any changes to build scripts or CI workflows occur (which can affect supply chain), then treat it like an infra change – require review. Also, run a quick static analysis on the workflow for insecure patterns (e.g. GitHub Actions with pull_request_target that could be abused).
•	If releasing a build (merge to main or a tagged release), then enforce:
•	SBOM generation: The build pipeline must produce an SBOM listing all components. If SBOM creation fails, then fail the release. (SBOM tools like Syft can generate SBOM; failure might indicate unusual build steps that need attention.)
•	Signature: After building the artifact (binary, container, etc.), the pipeline signs it (e.g. with cosign or GPG). If signing fails, then do not publish the release. The signature (and/or provenance attestation) is required to ensure deploy-time verification.
•	Provenance: Use a trusted CI context to generate provenance metadata (who built it, what sources, etc.). If provenance info is incomplete or indicates an untrusted build environment, then block release. (For instance, if not built on the official CI or the source wasn’t a merge commit from main, block it – this stops out-of-band builds.)
•	Final Vulnerability Check: Optionally, scan the final artifact (container image scan) for vulnerabilities. If any Critical vulns are present (that are not waived), then block deployment.
•	If an artifact is about to be deployed to production, then the deployment process must:
•	Verify the artifact’s signature against the expected public key.
•	Verify the artifact’s checksum matches what CI produced (to prevent tampering).
•	Verify that the artifact comes from a build that had all checks passing (could use provenance to ensure the build ID corresponds to a successful CI run).
•	If any of these verifications fail, then abort the deployment. Do not allow unverified code to run in prod.
•	If any step in the supply chain indicates anomaly (e.g. SBOM shows an unexpected binary blob, or an artifact isn’t reproducible), then flag for human investigation and stop the pipeline.
•	If the project is using dependencies with known issues but under waivers (tracked via waivers.yaml or similar), then at release time remind or require acknowledgment. (E.g., “Releasing with known vuln X in dependency Y under waiver – proceed? [Yes/No]”. Ideally the human must consciously hit yes.)
•	If an upstream dependency vulnerability is announced that affects the repo (outside of a PR, e.g. via Dependabot alerts), then mark future releases as blocked until it’s patched or explicitly waived. This ensures even out-of-cycle supply chain issues get handled.
•	If secrets are required for builds (like signing keys), then they must be stored securely (in CI secrets, not in repo). If any secret is detected in the repo (hardcoded API keys, etc.), then fail immediately (no waiver, as it violates absolute security).
•	If a third-party action or plugin is used in CI (e.g. a community GitHub Action), then pin it to a specific version or SHA. If a workflow uses an unpinned action version (floating @main), then warn or fail (to prevent supply chain attacks via CI).
•	If releasing a new version, then attach the SBOM and signatures to the release artifacts (so consumers can verify). Also, if possible, automatically diff SBOMs between releases and check for unexpected additions[17] (as an extra guard).
•	If any supply chain check fails and is non-waivable (e.g. artifact signature mismatch), then the system will not deploy – it’s a hard stop until resolved.
These rules collectively create a trust chain from code to deployment. They ensure that what goes into the software is known and vetted, and that the output is verifiable. For a solo dev, this might sound heavy, but modern tools have automated much of it (e.g., one-step SBOM + sign actions[17]). The idea is to catch things like the earlier mentioned malicious crates or npm worms at the PR stage and to guarantee that production only runs code that went through the AI’s supervised pipeline.
AI Behavioral Rules
•	If the AI proposes a change that would alter its own governance constraints (e.g. modifying the CI config to remove tests, or changing policy files to be laxer), then require human approval. The AI is forbidden from unilaterally loosening its restrictions. Such PRs should be flagged as “AI-self-governance change – manual review required.”
•	If the AI writes code that introduces a secret (like hardcoding a password or API key), then the secret scan in CI fails the build with no waiver allowed. Additionally, the incident is logged as an AI policy violation. The AI must remove or externalize the secret to proceed.
•	If the AI’s generated code triggers new security warnings (say the static analyzer finds an XSS vulnerability), then the PR is marked high risk. The AI should attempt to fix it, but if it cannot, a human review is forced. Given ~45% of AI code has security flaws[6], this rule is crucial: any security tool finding of High severity automatically stops auto-merge.
•	If the AI generates duplicate code or reintroduces a pattern that an ADR marked as deprecated, then the violation is caught (by duplicate detection or pattern checks) and fed back for correction. The AI is expected to follow project conventions; if it doesn’t, the pipeline fails as described in boundary rules. (E.g., if DECISIONS.md says “Using custom error handling X is deprecated,” an automated grep or AI check can catch if new code uses X and then fail the PR[16].)
•	If an AI-authored change lacks adequate tests or evidence, then it cannot be merged (reinforcing the earlier contract rule). For example, if the diff has >20 lines of new code and 0 lines of new tests, then block merge until tests are added. The AI is basically required to generate tests alongside code.
•	If the AI attempts to merge code without review (e.g., in a lapse, tries to push to main or bypass the PR process), then branch protection settings prevent it. All code must go through PRs with the contract and CI checks – the AI’s Git credentials or pipeline setup should not allow direct push to protected branches. Any such attempt should be logged as an error.
•	If the AI produces a PR that is too large or complex for safe review (say > X lines changed), then the system could warn or even fail it, advising to break into smaller changes. This is based on the notion that extremely large AI diffs are risky (as noted by human maintainers rejecting huge AI PRs[21]). A rule could be: If PR > 1000 lines and risk High, then require human review and possibly split.
•	If the AI continuously suggests changes that flip-flop or degrade quality (pattern of adding and then removing the same code, or causing test regressions repeatedly), then a meta-rule triggers: the pipeline or an AI overseer flags that the AI might be in a loop or drifting. Then require a human to intervene (perhaps adjust the AI’s instructions or retrain it on project context).
•	If any audit log (of AI prompts/decisions) is to be kept, then ensure it does not include sensitive data but captures enough context (e.g., the prompt that led to a weird code generation is stored). This aids debugging AI behavior. So, if an AI action occurs, then log: action type, trigger (commit, PR comment etc.), and outcome.
•	If the AI uses information outside the repo (e.g., browsing external resources for a solution), then that should be recorded (for transparency and license compliance). For instance, if it copied code from StackOverflow, ideally it should include attribution if required. The policy could be forbidden for AI to paste large snippets from unknown sources to avoid license pollution.
•	If the AI is working on a branch and the branch diverges significantly from main (out of date), then it should merge or rebase latest changes before proposing PR, to avoid it working off stale context. Essentially enforce that AI’s proposals are based on up-to-date code.
•	If the AI’s output starts to drift in style (not following formatting or naming conventions), then lint checks and style enforcers will catch it. The rule is simply: no style deviations allowed beyond what the formatter fixes. The AI is expected to adhere to the project’s coding style (autoformat tools ensure this).
•	If “silent degradation” of quality is detected – e.g., over 5 PRs, test coverage has gone from 90% to 70%, or performance benchmarks gradually worsened – then the system raises an alert. This might not be a single PR rule but an aggregate one. Still, it’s codified: if metric X falls below threshold or drops Y% from last release, then fail the quality gate or require human review of why. In practice: maintain a baseline and compare each release.
•	If the AI is configured with multiple models or tools, and one starts giving inconsistent results (pattern drift or increased hallucinations), then consider automatically switching or retraining. (This is more of a meta governance: e.g., if we detect the AI making many mistakes in a row, escalate to human to possibly adjust the AI’s parameters or prompts.)
In summary, these AI governance rules aim to prevent the AI from undermining the system either by bypassing controls or by gradually eroding quality without notice. The allowed actions for the AI (writing code, tests, docs, refactoring with guidance) are all within the sandbox of the policy framework. Forbidden actions (self-approving risky code, removing evidence, ignoring style, injecting secrets) are explicitly guarded by automated checks. The requirement for evidence on AI changes and full audit trails ties back to accountability – even though the AI is doing 100% of code changes, the human operator has full visibility and veto power when needed[22].
Failure Modes & Detection Signals
Despite robust controls, it’s important to anticipate how things might go wrong and how the system detects and responds. Below are potential failure modes and the detection signals that would catch them:
•	Misleading Change Description: The AI might produce a PR description that doesn’t accurately reflect the code changes (intentionally or accidentally). Detection: The contract vs diff check will flag any discrepancy. For instance, if the contract says “refactor only” but the diff adds a new feature, CI fails due to scope mismatch. The signal is an automated contract validator diffing declared scope against actual file changes – it will raise an error if they differ[5].
•	Insufficient Testing or Evidence: The AI could attempt to merge code that isn’t properly tested or proven. Detection: The required evidence rule prevents merge without tests/logs. If the AI forgot to include a new test, the contract check fails. If tests are included but failing, the test suite result is red, blocking CI. Essentially, the absence of green tests is an obvious signal; in addition, if coverage drops or no new tests accompany new code, a coverage gate or policy check will fail the PR.
•	Unauthorized High-Risk Change: An AI-generated change touches critical security or safety code, but tries to merge automatically. Detection: The risk classification mechanism and file path watchers catch this. For example, modifying auth.py triggers a tag “security-change”. The Gate Matrix logic then ensures auto-merge is off and requires human review[1]. If the AI somehow tries to bypass that (it shouldn’t, due to branch protections), the lack of an approved human review on a protected branch will block the merge (GitHub or the CI merge bot won’t merge without the required check).
•	Failure Hidden by Waiver Abuse: The AI could start slapping waivers on failing tests continuously to get builds green (since it doesn’t “feel” pain of tech debt). Detection: The waiver audit logs will reveal an unusual number of waivers. The system’s escalation rules will trigger if the same waiver repeats. For instance, if “Test123” was waived in 3 consecutive runs, the next run fails outright (no waiver allowed), thereby exposing the persistent failure. Also, periodic reports can list all active waivers; a human will see if the count is growing abnormally, which is a sign of accumulating unresolved issues.
•	Architectural Drift or Entropy: Over time, the AI might introduce subtle architecture violations (couplings, cycles) that individually were waived or unnoticed. Detection: The architecture enforcement is continuous – each PR’s static check would catch blatant violations. For drift, the system can run a scheduled architecture conformance check (maybe on each release). If the number of exceptions or suppressed violations is increasing, it raises a warning. AI agents can also detect drift by comparing current code structure to the intended model[23] – differences (like an extra dependency) are flagged. In short, drift is caught by either a failing rule on a PR or by a trend report of architecture metrics (e.g., layering violations count).
•	Duplicate or Dead Code Accumulation: The AI might inadvertently duplicate logic (a known common flaw)[18], creating parallel implementations or leaving unused generated code. Detection: A duplication scanner (like a simian or PMD CPD tool, or even an AI code analysis) runs in CI for each PR or release. If it finds a new duplication above threshold, it fails. Also, code coverage tools find dead code (lines not executed by any test) – a sudden increase in dead code could alert maintainers. The detection signal is either an explicit “duplication % increased” failing the quality gate, or reviewer AI saying “function X looks similar to Y”. Because our process requires tests and often the AI will write similar tests for duplicates, it might even catch itself when a test already exists.
•	Complexity Explosion: The AI might generate a very complex solution when a simpler one was needed, making maintenance harder (complexity creep). Detection: The complexity budgets are enforced by static analysis metrics. If a PR raises the cyclomatic complexity of a module beyond the allowed max, the complexity check fails. SonarQube or similar in CI would flag “Complexity increased by N, which is over the limit”, causing a non-green quality gate. That directly stops the merge. Additionally, any time a single function exceeds a set threshold, a linter can catch it immediately.
•	Release of Vulnerable Artifact: A scenario: the AI adds a dependency with a known vuln but merges it under a waiver or by oversight. Detection: The release-time scan of the SBOM and vulnerabilities acts as a backstop. Even if it slipped through PR (say the DB of vulns updated after merge), at release the pipeline runs an SCA on the full product. If it finds a Critical, it will halt the release. The signal is a vulnerability report showing the issue. Also, many repos use services that continuously monitor dependencies (e.g. Dependabot alerts). The OS would treat such an alert as a blocking issue for the next release until fixed or waived.
•	Unsigned or Tampered Artifact: Perhaps a signing step fails quietly or an artifact is altered. Detection: The deploy-time verification will catch mismatches. If an artifact isn’t signed with the expected key, the deployment script refuses it. If the hash doesn’t match the SBOM/provenance data, it’s not the same build output, so it’s rejected. These checks ensure that what gets deployed is exactly what CI produced, or else it’s a no-go. Any tampering would break the chain (signature invalid), a clear signal of integrity failure.
•	AI Silent Failure to Apply Policy: The AI might, for example, not incorporate a new coding rule in its suggestions (like still using an old pattern that an ADR deprecated). Detection: The pattern checks and tests will catch it. For example, if an ADR says “use new API X instead of Y”, and AI uses Y, perhaps tests will fail (if Y was removed) or an automated grep finds usage of Y. Another angle: if the AI’s output quality degrades (maybe due to model drift), we’ll see more test failures and need for waivers – which again triggers our earlier detection of waiver spikes or failing tests. So the signals are increases in pipeline failures, waiver counts, static analysis warnings, etc. Any sustained negative trend surfaces as either an automated alert or simply forces attention because things stop auto-merging.
•	Human Override Mistakes: The human might accidentally approve or merge something they shouldn’t (like misjudge a security risk). Detection: The system provides safety nets: even human-approved, the checks must be green or waived. If the human merged a failing build with a waiver, it’s logged. The next pipeline (like deploy) might still catch issues (e.g., a problematic change causing runtime errors triggers monitoring alerts). In a solo setup, it’s about the human noticing via logs or post-deploy checks. The OS can assist by summarizing “This release went out with 2 waivers: X, Y. Be aware.” So any oversight is at least documented.
•	External Attack on CI/Repo: (Edge case for solo dev) If an attacker tries to inject malicious code via a dependency or CI config change (as has happened via compromised actions[24]), detection: signing and policy checks help, but this is tricky. The OS would rely on code reviews of CI changes (which we mandated manual) and verifying artifact provenance (so a compromised CI node can’t produce a valid attestation, hopefully). Any divergence (like an artifact built outside official workflow) is flagged at deploy. While not foolproof, these controls raise the bar and log unusual events (like someone force pushing to main would break the defined process, sounding alarms).
In essence, every anticipated failure either stops the pipeline or produces a visible alert. The multi-layered checks (contract validation, static analysis, dynamic tests, audit logs, etc.) create overlapping nets that make silent failures unlikely. For example, an AI bug that sneaks past tests might be caught by a post-deploy monitor or by the AI itself in a later pass. The combination of preventative rules and detection signals (many of which are automated) ensures that the system either automatically handles the issue (rollback, block, etc.) or at least brings it to the solo operator’s attention promptly.
Adoption Roadmap (Phased Implementation)
Adopting this Diamond-Level OS in a solo project can be done iteratively. Here’s a phased roadmap to gradually introduce these controls without overwhelming the workflow:
Phase 1 – Baseline Instrumentation: Start by introducing the Change Contract and basic CI checks in a non-blocking way. 1. Implement PR Template: Add the PR contract template (intent, scope, etc.) to the repo. Require the AI (and yourself) to fill it for each PR. At first, do not fail CI if it’s missing, but log a warning – use this phase to get used to writing good contracts. 2. Set Up CI Pipeline with Key Checks: Configure automated tests, linters, and a simple script to validate that the PR description contains the contract fields. In Phase 1, configure checks to WARN on issues (e.g., print “scope mismatch” but don’t fail) so as to not block work immediately. 3. Collect Metrics: As changes flow, observe how often tests fail, how often scope mismatches occur, etc. This will highlight where the AI needs guidance. This phase is about instrumenting the process with minimal friction, establishing a baseline for future ratcheting.
Phase 2 – Enforce Essential Gates (Pilot Auto-Merge): Begin turning warnings into gating conditions for low-risk items, and enable auto-merge on the safest changes to build trust in automation. 1. Mandatory Contract & Evidence: Now mark the Change Contract check as required. The PR will not merge if the contract is missing or incomplete. Also mandate that at least one form of evidence (new test or manual verification) is present for any code change. This ensures the habit of evidence-first is in place. 2. Introduce Risk Labeling: Have the CI automatically label PRs or set a status based on risk (Low/Med/High) using simple heuristics (e.g., file path rules). Use this to inform everyone (you) what category a change is. You can adjust misclassifications manually at first. 3. Auto-Merge Docs & Trivial Fixes: Configure an automation (like GitHub auto-merge or a bot) to automatically merge documentation-only PRs and maybe typo fixes or purely comment changes, once checks pass. This is a low-risk trial of letting the system merge without human click. Monitor this closely to ensure it’s working as intended. 4. Human Review on High Risk: Conversely, start requiring that High risk labeled PRs cannot be merged without an explicit “reviewed” flag. In practice, as a solo dev, this might mean you have to add a comment like “Approved after review” or use an admin override if needed. It’s a simulation of formal review to get into the flow of treating those PRs specially. 5. Train the AI: At this phase, educate the AI on these processes. For example, include in its prompt instructions or examples that show filling out the PR contract and running tests is expected. The AI will adapt to these requirements (maybe even generate the contract text on its own, given examples).
Phase 3 – Expand Coverage & Waiver Control: Now that basic gating is working, extend the governance to more areas and formalize the waiver process. 1. Enable More CI Checks: Integrate static analysis (lint rules for architecture, security scans for secrets) into the pipeline. At first, set them to warn unless critical. For instance, add a secret scan that fails on finding secrets (critical), but set the architecture layer check to warn if violated (since there may be existing violations to address). 2. Formal Waiver Logging: Introduce the waivers.yaml (or a simple mechanism to log waivers in version control). When you encounter a flaky test or a false positive that you want to bypass, practice creating a waiver entry with justification. Adjust CI to read this file: e.g., if waivers.yaml has an entry for “TestXYZ on OS=Windows”, then skip failing that test. At this stage, allow waivers fairly freely but record them. 3. Waiver Policy Introduction: Define which things you will not allow yourself to waive. For example, decide “I will not waive security scan failures or critical tests.” Document this policy (maybe in the policy manifest). The idea is to start self-imposing limits. 4. Enforce Ratchets for New Issues: If a new type of check is added (say code complexity), use the ratchet approach: have it only warn for now, but set a calendar reminder or pipeline condition that “after 1 month, this warning becomes an error.” Communicate this in output so you and the AI know the countdown. 5. Manual Architecture Review: For any significant feature, do a mini architecture sanity check. Since the tools are warning about some layer issues, maybe fix a couple that are easy. Essentially, start aligning the code with the intended architecture diagram, so that by next phase the automated enforcement won’t break the build due to legacy issues.
Phase 4 – Full Enforcement & Advanced Automation: At this stage, aim for all critical rules to be auto-enforced (with fail gates), and incorporate supply chain security. 1. Strict Auto-Merge Rules: Allow auto-merge for all Low/Med risk changes that pass checks, not just docs. By now, tests and linters should be reliable such that if they’re green, you can trust a merge. Still keep High risk and special cases as manual. This will significantly speed up routine merges – essentially CI becomes the de-facto reviewer for most PRs. 2. Enforce “Never Auto” Categories: Implement the never-auto merge rules explicitly. For example, set a branch protection or bot rule: if a PR touches security/ or infrastructure/, label it “Needs Human” and do not enable auto-merge. You, as the human, then know to do an extra review on those. This solidifies the safety net for the riskiest changes. 3. Waiver Tightening: Now put limits on waivers in code: update the waiver handling script to fail if a waiver is missing expiry or if a waiver has been open > 30 days. Also implement the “3 strikes” rule: if the same waiver appears in 3 consecutive releases, block the 4th. Since you have logs from phase 3, you can identify if any issue is repeatedly waived and plan to fix it now. 4. Architectural Enforcement: Turn architecture linting from WARN to FAIL for new violations. By this time, you should have resolved or formally waived existing deviations. Integrate a tool or AI check to CI that will break the build on any layer rule violation that doesn’t have an accompanying ADR/waiver. This means the AI must respect the module boundaries or go through the exception process. 5. Supply Chain Integration: Incorporate SBOM generation in the CI pipeline for every build. Use a GitHub Action or similar to generate SBOM and possibly a vulnerability scan (you can use tools like Grype). Fail the build if any Critical vulns are found in new dependencies. Also set up artifact signing: e.g., generate a signing key (or use GitHub OIDC with Sigstore for automated keyless signing) to sign releases. Practice doing a signed release and verifying it locally. From now on, treat an unsigned artifact as untrustworthy – if signing fails in CI, fix it before proceeding. 6. Deploy Gate (if applicable): If you have an automated deployment (to a server or cloud), implement the gate that checks the signature and maybe runs a smoke test in production. While as a solo dev you might deploy manually, even a manual deploy can have a script that refuses to deploy unverified builds. Test this process to ensure it’s not too cumbersome.
Phase 5 – Continuous Governance and Improvement: Achieve “diamond” level where the system is finely tuned and require minimal human intervention except where intended. 1. Complete Automation for Low Risk: At this final phase, merging and releasing of low-risk changes could be 100% automated. For example, a documentation change PR could go from green CI to auto-merge to an automatic deployment (if CI/CD is fully set) without you doing anything. Monitor these automated flows to build confidence. The human (you) now mostly monitors dashboards and addresses alerts rather than manually merging code. 2. Periodic Audit & Metrics: Establish a routine (maybe monthly) to review metrics: number of waivers open, average time to fix high severity issues, test coverage trend, etc. If any metric is going in the wrong direction (say coverage down or waiver count up), use that as a trigger to adjust. This might involve adding a new rule or tightening a threshold. For instance, if code duplication creeps up, lower the allowed duplicate threshold or add a CI step to enforce refactoring sooner. 3. AI Model and Prompt Updates: As you see how the AI performs under these constraints, refine its prompting. Perhaps add instructions in its system prompt about architecture rules or where to find ADRs. Possibly incorporate an AI agent specifically to review PRs (as suggested in literature[25]) – this agent could comment on PRs with potential improvements or catch things early. Since the groundwork is laid, this is an enhancement: you can experiment with an AI “pair reviewer” in CI that gives an extra opinion on design or finds anomalies that static tools might miss (with the understanding that it’s advisory). 4. Expand Policies Based on Need: If new concerns arise (e.g., you start doing mobile development and need to enforce UI/UX guidelines, or you incorporate a new language needing its own lint rules), add those to the policy manifests and CI. The system is designed to be extensible – you can add a new check and decide if it’s warn or fail, and schedule its ratchet. 5. Community/Open-Source Ready: If the project becomes open source or others contribute, you already have a robust framework to handle contributions. At Diamond level, essentially any contributor (human or AI) is subject to the same automated scrutiny. The human operator’s workload scales minimally because the gates and contracts force contributors to provide evidence and follow structure. You might refine the process to handle external PRs (like ensuring forks can’t skip checks, etc., but that’s standard with CI).
Through these phases, the key is gradual tightening. In Phase 1 and 2, you avoid stalling development by making things advisory; by Phase 4 and 5, the system should be strict but by then the codebase and AI have adapted to the rules. Each phase delivers immediate benefits (e.g., Phase 2 already gives auto-merge for docs and reduces toil, Phase 3 logs waivers to build a risk memory, etc.) while building toward the end goal. Because you’re solo, you have the flexibility to adjust pacing – if at any point the friction seems too high, you can extend a phase or temporarily loosen a rule to keep momentum, then ratchet up again.
In the end, you’ll have a fully AI-driven software factory where policy is king. The code, the tests, the documentation, and the compliance artifacts all flow through an integrated pipeline of checks and balances. The human role becomes one of policy curator and risk overseer, rather than day-to-day code author. This roadmap ensures that the transition to that state is smooth, with each step validated and value-adding. The final outcome is a repository that essentially governs itself under the watchful eye of encoded best practices – the essence of a Diamond-Level AI-Native Repository OS.
________________________________________
[1] [4] [5] [6] [15] [18] [21] [22] AddyOsmani.com - AI writes code faster. Your job is still to prove it works.
https://addyosmani.com/blog/code-review-ai/
[2] [11] [13] [14] [19] [23] [25] Using AI Agents to Enforce Architectural Standards | by Dave Patten | Medium
https://medium.com/@dave-patten/using-ai-agents-to-enforce-architectural-standards-41d58af235a0
[3] [17] SBOM Tools: Drop an SBOM GitHub Action into your Workflow
https://anchore.com/sbom/sbom-tools-drop-sbom-action-in-github-actions/
[7] [8] [20] Automated SBOMs: Turning GitHub Actions into a Supply Chain Shield | made by uknowwhoab1r | Medium
https://medium.com/@md.abir1203/rust-crates-security-analysis-from-solo-dev-to-supply-chain-defender-b802255351fe
[9]  Streamline Compliance Audits with Waivers | Chef 
https://www.chef.io/blog/streamline-compliance-audits-with-waivers
[10] Waivers
https://help.sonatype.com/en/waivers.html
[12] [16] The Context Illusion: Why LLMs Don't Know Your Code Like You Think They Do
https://pullflow.com/blog/the-context-illusion-why-llms-dont-know-your-code-like-you-think-they-do
[24] Shai Hulud Attacks Persist Through GitHub Actions Vulnerabilities
https://www.aikido.dev/blog/github-actions-incident-shai-hulud-supply-chain-attack
